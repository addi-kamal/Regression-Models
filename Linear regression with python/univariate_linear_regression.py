# -*- coding: utf-8 -*-
"""Univariate Linear Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14q9tsfX0m-qi5YcDw1zPI5IzqFAWYuoF

**<h1 align="center"> Univariate Linear Regression </h1>**

### Task 1: Load the Data and Libraries
---
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt 
plt.style.use('ggplot')
# %matplotlib inline

import numpy as np
import pandas as pd  
import seaborn as sns 
plt.rcParams['figure.figsize'] = (12, 8)

My_Data = pd.read_csv("bike_sharing_data.txt")

"""### Task 2: Understand the Data
---
"""

My_Data.head()

My_Data.info()

My_Data.describe()

"""### Task 3: Visualize the Data
---

Before starting on any task, it is often useful to understand the data by visualizing it.

For this dataset, we can use a scatter plot using Seaborn to visualize the data, since it has only two variables: the profit and population.
"""

ax = sns.scatterplot(x = "Population", y = "Profit", data = My_Data, color = "blue")
ax.set_title("Profit vs City Population");

"""### Task 4: Compute the Cost $J(\theta)$
---

Letâ€™s now take a look at the machinery that powers linear regression: Gradient Descent. 

We want to fit the linear regression parameters ğœƒ to our dataset using gradient descent.

We can think of the cost as the error your model made in estimating a value.

The objective of linear regression is to minimize the cost function J(ğœƒ) :

$$J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)} )^2$$

where $h_{\theta}(x)$ is the hypothesis and given by the linear model

$$h_{\theta}(x) = \theta^Tx = \theta_0 + \theta_1x_1$$
"""

def Cost_Function(x, y, theta):
  m = len(y)
  y_pred = x.dot(theta)
  error = (y_pred-y)**2
  return 1/(2*m)*np.sum(error)

m = My_Data.Population.values.size
X = np.append(np.ones((m, 1)), My_Data.Population.values.reshape(m, 1), axis = 1)
Y = My_Data.Profit.values.reshape(m, 1)
theta = np.zeros((2, 1))
Cost_Function(X, Y, theta)

"""### Task 5: Gradient Descent
---

Recall that the parameters of our model are the ğœƒ_j values.
These are the values we will adjust to minimize the cost J(ğœƒ).
One way to do this is to use the batch gradient descent algorithm.
In batch gradient descent, each iteration performs the following update.
With each step of gradient descent, the parameters ğœƒ_j come closer to the optimal values that will achieve the lowest cost J(ğœƒ).

Minimize the cost function $J(\theta)$ by updating the below equation and repeat unitil convergence
        
$\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$ (simultaneously update $\theta_j$ for all $j$).
"""

def Gradient_Descent(X, Y, theta, alpha, iterations):
  m = len(Y)
  costs = []
  for i in range(iterations):
    Y_pred = X.dot(theta)
    error = np.dot(X.transpose(), (Y_pred-Y))
    theta -= alpha*(1/m)*error
    costs.append(Cost_Function(X, Y, theta))
  return theta, costs

theta, costs = Gradient_Descent(X, Y, theta, alpha = 0.01, iterations = 2000)

print("hÎ¸(x) = {} + {}x1".format(str(round(theta[0, 0], 2)),
                                 str(round(theta[1, 0], 2))))

"""### Task 6: Visualising the Cost Function $J(\theta)$
---

To better understand the cost function J(ğœƒ), we will plot the cost over a 2-dimensional grid of ğœƒ_0 and ğœƒ_1 values.
"""

from mpl_toolkits.mplot3d import Axes3D

theta_0 = np.linspace(-10, 10, 100)
theta_1 = np.linspace(-1, 4, 100)

cost_value = np.zeros((len(theta_0), len(theta_1)))

for i in range(len(theta_0)):
  for j in range(len(theta_1)):
    t = np.array([theta_0[i], theta_1[j]])
    cost_value[i, j] = Cost_Function(X, Y, t)

fig = plt.figure(figsize = (12, 8))
ax = fig.gca(projection = "3d")

surf = ax.plot_surface(theta_0, theta_1, cost_value, cmap = "viridis")
fig.colorbar(surf, shrink = 0.5, aspect = 5)

plt.xlabel("$\Theta_0$")
plt.ylabel("$\Theta_1$")
ax.set_zlabel("$J(\Theta)$")
ax.view_init(30, 330)

plt.show()

"""The purpose of this graph is to show you how J(ğœƒ) varies with changes in ğœƒ_0 and ğœƒ_1.
We can see that the cost function J(ğœƒ) is bowl-shaped and has a global minimum.

### Task 7: Plotting the Convergence
---

Letâ€™s plot how the cost function varies with the number of iterations.
When we ran gradient descent previously, it returns the history of J(ğœƒ) values in a vector â€œcostsâ€.
We will now plot the J values against the number of iterations.

Plot $J(\theta)$ against the number of iterations of gradient descent:
"""

plt.plot(costs)
plt.xlabel("Iterations")
plt.ylabel("$J(\Theta)$")
plt.title("Values of the cost function")

"""### Task 8: Training Data with Linear Regression Fit
---

Now that we have correctly implemented and run gradient descent and arrived at the final parameters of our model, we can use these parameters to plot the linear fit.
"""

theta.shape

theta

theta = np.squeeze(theta)
sns.scatterplot(x = "Population", y = "Profit", data = My_Data, color = "blue")

x_value = [x for x in range(5, 25)]
y_value = [(x*theta[1]+theta[0]) for x in x_value]
sns.lineplot(x_value, y_value)

plt.xlabel("Population in 10000s")
plt.ylabel("Profit in $10000s")
plt.title("Linear regression Fit");

"""### Task 9: Inference using the optimized $\theta$ values
---

In this final task, letâ€™s use our final values for ğœƒ to make predictions on profits in cities of 35,000 and 70,000 people.

$h_\theta(x) = \theta^Tx$
"""

def predict(x, theta):            #hypothesis
  y_pred = np.dot(theta.transpose(), x)
  return y_pred

y_pred1 = predict(np.array([1, 4]), theta)*10000
print("For a population of 40000 people the model predicts a profit of "+ str(round(y_pred1, 0)))

y_pred2 = predict(np.array([1, 8.3]), theta)*10000
print("For a population of 40000 people the model predicts a profit of "+ str(round(y_pred2, 0)))